{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SURE library use case notebook\n",
    "Useful links:\n",
    "- [Github repo](https://github.com/Clearbox-AI/SURE)\n",
    "- [Documentation](https://dario-brunelli-clearbox-ai.notion.site/SURE-Documentation-2c17db370641488a8db5bce406032c1f)\n",
    "\n",
    "Download the datasets and try out the library with this guided use case.\n",
    "\n",
    "We would greatly appreciate your feedback to help us improve the library! \\\n",
    "If you encounter any issues, please open an issue on our [GitHub repository](https://github.com/Clearbox-AI/SURE).\n",
    "\n",
    "### Datasets description\n",
    "\n",
    "The three datasets provided are the following:\n",
    "\n",
    "- *census_dataset_training.csv* \\\n",
    "    The original real dataset used to train the generative model from which *census_dataset_synthetic* was produced.\n",
    "    \n",
    "- *census_dataset_validation.csv* \\\n",
    "    This dataset was also part of the original real dataset, but it was NOT used to train the generative model that produced *census_dataset_synthetic*.\n",
    "    \n",
    "- *census_dataset_synthetic.csv* \\\n",
    "    The synthetic dataset produced with the generative model trained on *census_dataset_training.*\n",
    "    \n",
    "\n",
    "The three census datasets include various demographic, social, economic, and housing characteristics of individuals. Every row of the datasets corresponds to an individual.\n",
    "\n",
    "The machine learning task related to these datasets is a classification task, where, based on all the features, a ML classifier model must decide whether the individual earns more than 50k dollars per year (label=1) or less (label=0).\\\n",
    "The column \"label\" in each dataset is the ground truth for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installing the library and importing dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the SURE library \n",
    "%pip install clearbox-sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import polars as pl # you can use polars or pandas for importing the datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sure import Preprocessor, report\n",
    "from sure.utility import (compute_statistical_metrics, compute_mutual_info,\n",
    "\t\t\t  \t\t\t  compute_utility_metrics_class,\n",
    "\t\t\t\t\t\t  detection,\n",
    "\t\t\t\t\t\t  query_power)\n",
    "from sure.privacy import (distance_to_closest_record, dcr_stats, number_of_dcr_equal_to_zero, validation_dcr_test, \n",
    "\t\t\t              adversary_dataset, membership_inference_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset import and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://raw.githubusercontent.com/Clearbox-AI/SURE/main/examples/data/census_dataset\"\n",
    "\n",
    "real_data  = pl.from_pandas(pd.read_csv(os.path.join(file_path,\"census_dataset_training.csv\"))).lazy()\n",
    "valid_data = pl.from_pandas(pd.read_csv(os.path.join(file_path,\"census_dataset_validation.csv\"))).lazy()\n",
    "synth_data = pl.from_pandas(pd.read_csv(os.path.join(file_path,\"census_dataset_synthetic.csv\"))).lazy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Datasets preparation\n",
    "Apply a series of transformations to the raw dataset to prepare it for the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor initialization and query execution on the real, synthetic and validation datasets\n",
    "preprocessor            = Preprocessor(real_data, num_fill_null='forward', scaling='standardize')\n",
    "\n",
    "real_data_preprocessed  = preprocessor.transform(real_data)\n",
    "valid_data_preprocessed = preprocessor.transform(valid_data)\n",
    "synth_data_preprocessed = preprocessor.transform(synth_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Statistical properties and mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistical properties and features mutual information\n",
    "num_features_stats, cat_features_stats, temporal_feat_stats = compute_statistical_metrics(real_data, synth_data)\n",
    "corr_real, corr_synth, corr_difference                      = compute_mutual_info(real_data_preprocessed, synth_data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 ML utility - Train on Synthetic Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the machine learning utility of the synthetic dataset on the classification task\n",
    "\n",
    "# ML utility: TSTR - Train on Synthetic, Test on Real\n",
    "X_train      = real_data_preprocessed.drop(\"label\") # Assuming the datasets have a “label” column for the machine learning task they are intended for\n",
    "y_train      = real_data_preprocessed[\"label\"]\n",
    "X_synth      = synth_data_preprocessed.drop(\"label\")\n",
    "y_synth      = synth_data_preprocessed[\"label\"]\n",
    "X_test       = valid_data_preprocessed.drop(\"label\").limit(10000) # Test the trained models on a portion of the original real dataset (first 10k rows)\n",
    "y_test       = valid_data_preprocessed[\"label\"].limit(10000)\n",
    "TSTR_metrics = compute_utility_metrics_class(X_train, X_synth, X_test, y_train, y_synth, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Detection Score\n",
    "Computes the detection score by training an XGBoost model to differentiate between original and synthetic data. \n",
    "\n",
    "The lower the model's accuracy, the higher the quality of the synthetic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_score = detection(real_data_preprocessed, synth_data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Power\n",
    "Generates and runs queries to compare the original and synthetic datasets.\n",
    "\n",
    "This method creates random queries that filter data from both datasets.\n",
    "\n",
    "The similarity between the sizes of the filtered results is used to score the quality of the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_power_score = query_power(real_data, synth_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Privacy assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Distance to closest record (DCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distances to closest record between the synthetic dataset and the real dataset\n",
    "# and the distances to closest record between the synthetic dataset and the validation dataset\n",
    "\n",
    "dcr_synth_train = distance_to_closest_record(\"synth_train\", synth_data, real_data)\n",
    "dcr_synth_valid = distance_to_closest_record(\"synth_val\", synth_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any clones shared between the synthetic and real datasets (DCR=0).\n",
    "\n",
    "dcr_zero_synth_train  = number_of_dcr_equal_to_zero(\"synth_train\", dcr_synth_train)\n",
    "dcr_zero_synth_valid  = number_of_dcr_equal_to_zero(\"synth_val\", dcr_synth_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some general statistcs for the DCR arrays computed above\n",
    "\n",
    "dcr_stats_synth_train = dcr_stats(\"synth_train\", dcr_synth_train)\n",
    "dcr_stats_synth_valid = dcr_stats(\"synth_val\", dcr_synth_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the share of records that are closer to the training set than to the validation set\n",
    "\n",
    "share = validation_dcr_test(dcr_synth_train, dcr_synth_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Membership Inference Attack test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a Membership inference Attack on your syntehtic dataset\n",
    "# To do so, you'll need to produce an adversary dataset and some labels as adversary guesses groundtruth\n",
    "\n",
    "# The label is automatically produced by the function adversary_dataset and is added as a column named \n",
    "# \"privacy_test_is_training\" in the adversary dataset returned\n",
    "\n",
    "# ML privacy attack sandbox initialization and simulation\n",
    "adversary_df = adversary_dataset(real_data_preprocessed, valid_data_preprocessed)\n",
    "\n",
    "# The function adversary_dataset adds a column \"privacy_test_is_training\" to the adversary dataset, indicating whether the record was part of the training set or not\n",
    "adversary_guesses_ground_truth = adversary_df[\"privacy_test_is_training\"] \n",
    "MIA = membership_inference_test(adversary_df, synth_data_preprocessed, adversary_guesses_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility-Privacy report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the utility privacy report with the information computed above\n",
    "\n",
    "report(real_data, synth_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test_sure)",
   "language": "python",
   "name": "test_sure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
